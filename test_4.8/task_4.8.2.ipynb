{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7d39f3",
   "metadata": {},
   "source": [
    "## Создание сессии Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40820be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .appName(\"task_47\")\\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.executor.cores\", 5)\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4004ddf",
   "metadata": {},
   "source": [
    "## Задание 4.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a867b",
   "metadata": {},
   "source": [
    "#### Исходные данные задания -> Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a494a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as DT\n",
    "import pandas as pd\n",
    "\n",
    "start_date = DT.datetime(2023, 8, 1)\n",
    "end_date = DT.datetime(2023, 8, 31)\n",
    "\n",
    "res = pd.date_range(\n",
    "    min(start_date, end_date),\n",
    "    max(start_date, end_date)\n",
    ").strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "week_tmp = [DT.datetime.strptime(i, '%Y-%m-%d').date().isocalendar()[1] for i in res]\n",
    "week_min = min(week_tmp)\n",
    "weeks = [str(i - week_min + 1) for i in week_tmp]\n",
    "res_days_p = tuple(zip(res, weeks))\n",
    "\n",
    "data_schema = ['day', 'week']\n",
    "res_days_s = spark.createDataFrame(data = res_days_p, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68382f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_days_s.printSchema()\n",
    "res_days_s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0341d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_str_p = (('1', '01.08—06.08'), ('2', '07.08—13.08'), ('3', '14.08—20.08'), ('4', '21.08—27.08'), ('5', '28.08—31.08'))\n",
    "data_schema = ['week', 'week_str']\n",
    "week_str_s = spark.createDataFrame(data = week_str_p, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_str_s.printSchema()\n",
    "week_str_s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dde7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_p = (('1', '01', 100), ('1', '02', 110), ('2', '01', 120), ('2', '02', 90), ('3', '01', 70), ('3', '02', 80))\n",
    "data_schema = ['product', 'location', 'demand']\n",
    "demand_s = spark.createDataFrame(data = demand_p, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_s.printSchema()\n",
    "demand_s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_p = (('1', '01', 1000), ('1', '02', 400), ('2', '01', 300), ('2', '02', 250))\n",
    "data_schema = ['product', 'location', 'stock']\n",
    "stock_s = spark.createDataFrame(data = stock_p, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79992ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_s.printSchema()\n",
    "stock_s.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e3225",
   "metadata": {},
   "source": [
    "#### Решение задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_days_s.createOrReplaceTempView('res_days')\n",
    "week_str_s.createOrReplaceTempView('week_str')\n",
    "demand_s.createOrReplaceTempView('demand')\n",
    "stock_s.createOrReplaceTempView('stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "with tmp_1 as (\n",
    "select d.*, coalesce(s.stock, 0) as stock\n",
    "from demand d \n",
    "    left join stock s using(product, location)\n",
    "),\n",
    "\n",
    "tmp_2 as (\n",
    "select tmp_1.*, res_days.*\n",
    "from tmp_1 cross join res_days\n",
    "),\n",
    "\n",
    "tmp_3 as (\n",
    "select tmp_2.*,\n",
    "    sum (demand) over (partition by product, location order by day) as dem\n",
    "from tmp_2\n",
    "),\n",
    "\n",
    "tmp_4 as (\n",
    "select tmp_3.*,\n",
    "    lag(stock - dem, 1) over (partition by product, location order by day) as clm\n",
    "from tmp_3\n",
    "),\n",
    "\n",
    "tmp_5 as (\n",
    "select tmp_4.*,\n",
    "    case \n",
    "        when clm > demand then demand\n",
    "        when clm > 0 and clm <= demand then clm\n",
    "        else 0\n",
    "    end as res\n",
    "from tmp_4\n",
    "),\n",
    "\n",
    "tmp_6 as (\n",
    "select tmp_5.*,\n",
    "    min(tmp_5.clm) over (partition by tmp_5.product, tmp_5.location, tmp_5.week) as clm_1\n",
    "from tmp_5\n",
    ")\n",
    "\n",
    "select \n",
    "    w.week_str,\n",
    "    tmp_6.product,\n",
    "    tmp_6.location,\n",
    "    sum(tmp_6.res),\n",
    "    avg(tmp_6.clm_1)\n",
    "from tmp_6\n",
    "    join week_str w on tmp_6.week = w.week \n",
    "group by w.week_str, tmp_6.product, tmp_6.location\n",
    "order by 1, 2, 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7248ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Итоговая таблица: \\n\\n')\n",
    "res = spark.sql(sql)\\\n",
    "    .withColumnRenamed('week_str', 'week_dates')\\\n",
    "    .withColumnRenamed('sum(res)', 'sales')\\\n",
    "    .withColumnRenamed('avg(clm_1)', 'stock_at_end')\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
