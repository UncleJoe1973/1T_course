{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753309b7",
   "metadata": {},
   "source": [
    "## Загрузка и сохранение файла исходных данных в формат csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02979b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d044131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = pd.read_excel(r'https://github.com/sultanmurad/spark_example_files/raw/main/online_retail.xlsx', sheet_name='Online Retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e27c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_r.shape)\n",
    "print(df_r.info())\n",
    "print(df_r.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.to_csv('online_retail.csv', index= False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79169148",
   "metadata": {},
   "source": [
    "## Создание сессии Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549510cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9af7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .appName(\"task_47\")\\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.executor.cores\", 5)\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [\n",
    "               StructField('InvoiceNo', StringType(), True),\n",
    "               StructField('StockCode', StringType(), True),\n",
    "               StructField('Description', StringType(), True),\n",
    "               StructField('Quantity', IntegerType(), True),\n",
    "               StructField('InvoiceDate', DateType(), True),\n",
    "               StructField('UnitPrice', DoubleType(), True),\n",
    "               StructField('CustomerID', StringType(), True),\n",
    "               StructField('Country', StringType(), True),\n",
    "            ]\n",
    "\n",
    "final_struc = StructType(fields = data_schema)\n",
    "\n",
    "df_s = spark.read.csv('online_retail.csv', sep=\";\", header=True, schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3d3fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_s.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6523958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ee534",
   "metadata": {},
   "source": [
    "## Анализ DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows count: ', df_s.count()) #число строк в файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc21d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Число уникальных покупателей\n",
    "df_s.select(f.count_distinct(df_s.CustomerID).alias('Число уникальных покупателей')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#временная таблица для расчетов\n",
    "temp_table_name = 'temp'\n",
    "df_s.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#В какой стране совершается большинство покупок?\n",
    "sql = \"\"\"\n",
    "    select distinct country\n",
    "    from temp\n",
    "    group by country\n",
    "    having count(CustomerID) = (select count(CustomerID) from temp group by country order by 1 desc limit 1)\n",
    "\n",
    "\"\"\"\n",
    "df = spark.sql(sql).withColumnRenamed('country', 'Cтрана с большинством покупок')\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b83aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Даты самой ранней и самой последней покупки на платформе\n",
    "\n",
    "# 1. Функции Spark\n",
    "df_s.select(f.min(df_s.InvoiceDate).alias('Минимальная дата'),\\\n",
    "            f.max(df_s.InvoiceDate).alias('Максимальная дата'))\\\n",
    ".show(truncate=False)\n",
    "\n",
    "# 2. Запрос Spark SQL\n",
    "sql = \"\"\"\n",
    "    SELECT min(InvoiceDate), max(InvoiceDate)\n",
    "    FROM temp\n",
    "\"\"\"\n",
    "df = spark.sql(sql)\\\n",
    "    .withColumnRenamed('min(InvoiceDate)', 'Минимальная дата')\\\n",
    "    .withColumnRenamed('max(InvoiceDate)', 'Максимальная дата')\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84aef50",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## RFM-анализ клиентов платформы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#поскольку данные имеют размах по дате 2010-12-01 - 2011-12-09, \n",
    "#для анализа предполагаю, что его дата - максимальная дата в датасете + 1 день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT\n",
    "  InvoiceNo\n",
    "  , InvoiceDate\n",
    "  , CustomerID\n",
    "  , SUM(Quantity*UnitPrice) AS Price\n",
    "  , MAX(InvoiceDate) OVER() + INTERVAL '1' DAY  AS Now\n",
    "FROM \n",
    "  temp\n",
    "WHERE \n",
    "  CustomerID IS NOT NULL\n",
    "GROUP BY \n",
    "  InvoiceNo\n",
    "  , InvoiceDate\n",
    "  , CustomerID\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(sql)\n",
    "\n",
    "df.createOrReplaceTempView('orders')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b399e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#расчет Recency, Frequency и Monetary за последний год (2011) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT \n",
    "  CustomerID\n",
    "  , MIN(datediff(Now, InvoiceDate)) AS Recency\n",
    "  , COUNT(DISTINCT InvoiceNo) AS Frequency\n",
    "  , SUM(Price) AS Monetary\n",
    "FROM\n",
    "  orders\n",
    "WHERE \n",
    "    InvoiceDate >= Now - INTERVAL '365' DAY\n",
    "GROUP BY \n",
    "  CustomerID\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(sql)\n",
    "\n",
    "df.createOrReplaceTempView('base')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3070e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#расчет границ групп Recency, Frequency и Monetary за последний год (по перцентилям - 33%, 67%, 100%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT\n",
    "     percentile(Recency, 0.33) AS R_33\n",
    "    , percentile(Recency, 0.67) AS R_67\n",
    "\n",
    "    , percentile(Frequency, 0.33) AS F_33\n",
    "    , percentile(Frequency, 0.67) AS F_67\n",
    "\n",
    "    , percentile(Monetary, 0.33) AS M_33\n",
    "    , percentile(Monetary, 0.67) AS M_67\n",
    "\n",
    "FROM \n",
    "    base\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(sql)\n",
    "\n",
    "df.createOrReplaceTempView('bins')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52aacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "\n",
    "WITH rfm AS(\n",
    "SELECT\n",
    " CustomerID\n",
    "  , Recency\n",
    "  , Frequency\n",
    "  , Monetary\n",
    "  , CASE \n",
    "         WHEN Recency <= R_33 THEN 'С'\n",
    "         WHEN Recency <= R_67 THEN 'B'\n",
    "         ELSE 'A'\n",
    "    END AS R\n",
    " , CASE \n",
    "         WHEN Frequency <= F_33 THEN 'A'\n",
    "         WHEN Frequency <= F_67 THEN 'B'\n",
    "         ELSE 'С'\n",
    "    END AS F\n",
    "   , CASE \n",
    "         WHEN Monetary <= M_33 THEN 'A'\n",
    "         WHEN Monetary <= M_67 THEN 'B'\n",
    "         ELSE 'С'\n",
    "    END AS M \n",
    "FROM\n",
    "  base\n",
    "CROSS JOIN \n",
    "  bins\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "    , Concat(r,f,m ) AS RFM_Score\n",
    "FROM \n",
    "    rfm\n",
    "ORDER BY \n",
    "    CustomerID\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(sql)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = df.select(['CustomerID']).filter(df.RFM_Score == 'AAA')\n",
    "df_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59702e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#из-за ошибки Py4JJavaError (которая вызвана, скорее всего, моей работой в Jupyter Notebook под Windows и\n",
    "#решения которой внятным способом я не обнаружил),\n",
    "#для записи результирующий DataFrame Spark конвертирую в Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49cc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_d = df_r.toPandas()\n",
    "p_d['CustomerID'] = p_d['CustomerID'].apply(lambda x: x.split('.')[0])\n",
    "p_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2661a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#запись результата в файл\n",
    "p_d.to_csv('result.csv', index= False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf199342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
